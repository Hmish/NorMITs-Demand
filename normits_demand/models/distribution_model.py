# -*- coding: utf-8 -*-
"""
Created on Tue Jun 18 12:17:07 2019
Updated on: Wednesday Sept 9th 2021


@author: cruella
Last update made by: Nirmal Kumar

File purpose:
Script to distribute the Commute productions generated by the production model
using the census journey to work data reformatted to model dimensions
"""
# Allow class self type hinting
from __future__ import annotations

# Builtins
import os  # File ops
import warnings

from typing import Dict
from typing import Tuple
from typing import Union
from typing import Any
from typing import List

# Third party imports
import pandas as pd

# local imports
import normits_demand as nd
import normits_demand.build.tms_pathing as tms

from normits_demand.concurrency import multiprocessing as mp
from normits_demand.distribution import gravity_model as gm  # For distribution functions
from normits_demand.utils import utils as nup  # Folder management, reindexing, optimisation


class DistributionModel(tms.TMSPathing):
    pass

    def path_config(self,
                    file_drive: nd.PathLike,
                    model_name: str,
                    iteration: str,
                    trip_origin: str = 'hb') -> Tuple[Dict[str, Union[Union[str, bytes], Any]],
                                                      Dict[Union[str, Any], Union[str, Any]]]:

        """
        Sets paths for imports/exports

        Assigns paths for various import parameters needed.
        Creates project folders.
        Returns import and export parameters as separate dictionaries

        Parameters
        ----------
        file_drive:
            Name of root drive to do work on.

        model_name:
            Name of model as string. Should be same as model descriptions.

        iteration:
            Current iteration of model. Defaults to global default.

        trip_origin:
            Type of trip origin. Whether home based or non home based.

        Returns
        ----------
        [0] imports:
            Paths to all Synthesiser import parameters.

        [1] exports:
            Paths to all Synthesiser output parameters
        """

        # Set base dir
        home_path = os.path.join(file_drive)

        # Set synth import folder
        import_path = os.path.join(home_path, 'import')

        # Set top level model folder, leave the slash on
        model_path = os.path.join(home_path, model_name, iteration)
        model_path += os.path.sep

        # Set model lookups location
        model_lookup_path = os.path.join(home_path, model_name, 'Model Zone Lookups')

        # Set production path, leave slash on
        production_path = os.path.join(model_path, 'Production Outputs')
        production_path += os.path.sep

        # Set production path
        production_path = (model_path +
                           'Production Outputs/')

        # Set attraction path
        attraction_path = (model_path +
                           'Attraction Outputs/')

        # Production import path
        if trip_origin == 'hb':
            p_import_path = (production_path +
                             model_name.lower() +
                             '_hb_internal_productions.csv')
            a_import_path = (attraction_path +
                             model_name.lower() +
                             '_hb_internal_attractions.csv')
        elif trip_origin == 'nhb':
            p_import_path = (production_path +
                             model_name.lower() +
                             '_nhb_internal_productions.csv')
            a_import_path = (attraction_path +
                             model_name.lower() +
                             '_nhb_internal_attractions.csv')
        # Raise user warning if no productions by this name
        if not os.path.exists(p_import_path):
            warnings.warn('No productions in folder.' +
                          'Check path or run production model')

        # Raise user warning if no productions by this name
        if not os.path.exists(a_import_path):
            warnings.warn('No attractions in folder.' +
                          'Check path or run attraction model')

        # Create project folders
        distribution_path = os.path.join(model_path, 'Distribution Outputs')
        nup.create_folder(distribution_path, chDir=False)

        fusion_path = os.path.join(model_path, 'Fusion Outputs')
        nup.create_folder(fusion_path, chDir=False)

        pcu_path = os.path.join(model_path, 'PCU Outputs')
        nup.create_folder(pcu_path)

        # Set distribution outputs (synthetic)
        reports = os.path.join(distribution_path, 'Logs & Reports')
        nup.create_folder(reports)

        summary_matrix_export = os.path.join(distribution_path, '24hr PA Distributions')
        nup.create_folder(summary_matrix_export)

        cjtw_hb_export = os.path.join(distribution_path, 'Cjtw PA Distributions')
        nup.create_folder(cjtw_hb_export)

        external_export = os.path.join(distribution_path, 'External Distributions')
        nup.create_folder(external_export)

        bin_export = os.path.join(distribution_path, 'Trip Length Distributions')
        nup.create_folder(bin_export)

        pa_export = os.path.join(distribution_path, 'PA Matrices')
        nup.create_folder(pa_export)

        pa_export_24 = os.path.join(distribution_path, 'PA Matrices 24hr')
        nup.create_folder(pa_export_24)

        arrival_export = os.path.join(distribution_path, 'D Arrivals')
        nup.create_folder(arrival_export)

        od_export = os.path.join(distribution_path, 'OD Matrices')
        nup.create_folder(od_export)

        me_export = os.path.join(distribution_path, 'PostME OD Matrices')
        nup.create_folder(me_export)

        compiled_pa_export = os.path.join(distribution_path, 'Compiled PA Matrices')
        nup.create_folder(compiled_pa_export)

        compiled_od_export = os.path.join(distribution_path, 'Compiled OD Matrices')
        nup.create_folder(compiled_od_export)

        # Set fusion exports
        fusion_summary_export = os.path.join(fusion_path, '24hr Fusion PA Distributions')
        nup.create_folder(fusion_summary_export)

        fusion_pa_export = os.path.join(fusion_path, 'Fusion PA Matrices')
        nup.create_folder(fusion_pa_export)

        fusion_od_export = os.path.join(fusion_path, 'Fusion OD Matrices')
        nup.create_folder(fusion_od_export)

        pcu_od_export = os.path.join(pcu_path, 'PCU OD Matrices')
        nup.create_folder(pcu_od_export)

        compiled_fusion_pa_export = os.path.join(fusion_path, 'Compiled Fusion PA Matrices')
        nup.create_folder(compiled_fusion_pa_export)

        compiled_fusion_od_export = os.path.join(fusion_path, 'Compiled Fusion OD Matrices')
        nup.create_folder(compiled_fusion_od_export)

        # Compile into import and export
        imports = {'imports': import_path,
                   'lookups': model_lookup_path,
                   'production_import': p_import_path,
                   'attraction_import': a_import_path}

        exports = {'production_export': production_path,
                   'attraction_export': attraction_path,
                   'reports': reports,
                   'summaries': summary_matrix_export,
                   'cjtw': cjtw_hb_export,
                   'external': external_export,
                   'tld': bin_export,
                   'pa': pa_export,
                   'pa_24': pa_export_24,
                   'od_export': od_export,
                   'arrival_export': arrival_export,
                   'me_export': me_export,
                   'c_pa_export': compiled_pa_export,
                   'c_od_export': compiled_od_export,
                   'fusion_summaries': fusion_summary_export,
                   'fusion_pa_export': fusion_pa_export,
                   'fusion_od_export': fusion_od_export,
                   'pcu_od_export': pcu_od_export,
                   'c_fusion_pa_export': compiled_fusion_pa_export,
                   'c_fusion_od_export': compiled_fusion_od_export}

        return imports, exports

    def distribute_cjtw(
            self,
            internal_24hr_productions,
            model_lookup_path,
            ia_name,
            model_name,
            calib_params,
            cost_type='24hr',
            subset=None,
            verbose=True):
        """
        This distributes commute productions by census journey to work distributions
        using zone to zone movements as a factor.

        Parameters
        ----------
        internal_24hr_productions:
            Commute only productions, should be pre-filtered before coming into
            function.

        ia_name:
            The name of the internal area of the model in use. Required for
            ensuring the right columns come through.

        model_name:
            Name of model. From global definition

        distribution_segments:
            Segments to distribute to. From distribution model definition.

        model_lookup_path:
            A model folder to pass to get_cjtw to find a census journey to work
            distribution.

        subset:
            Takes a vector of model zones to filter by. Mostly for test model runs.

        verbose:
            Indicates whether to print a log of the process to the terminal.
            Useful to set verbose=False when using multi-threaded loops.
            Defaults to True.

        Returns
        ----------
        [0] commute_pa:
            A 24hr commute matrix distributed by census journey to work, internal
            only, wide format.

        [1] d_bin:
            Trip length distribution bins (km) for cjtw distribution
        """

        # Get unique internal zones in a smart way
        min_zone = min(internal_24hr_productions['p_zone'])
        max_zone = max(internal_24hr_productions['p_zone'])
        unq_internal_zones = [i for i in range(min_zone, max_zone + 1)]

        # Filter to calib params - should be p1 only
        target_p = nup.filter_pa_cols(internal_24hr_productions,
                                      'p_zone',
                                      calib_params,
                                      round_val=3,
                                      verbose=verbose)[0]
        target_p = target_p.rename(
            columns={list(target_p)[-1]: 'productions'})

        target_p = target_p.rename(columns={'p_zone': ia_name})

        # Get productions before
        total_commute_productions = target_p['productions'].sum()

        # Get census journey to work
        cjtw = nup.get_cjtw(model_lookup_path,
                            model_name,
                            subset=subset)

        # Get area of residence col name
        aor_col = list(cjtw)[0]
        # Get area of workplace col name
        aow_col = list(cjtw)[1]
        # Change mode desc col to int for join
        # TODO: I would like this type change to be done in the function call
        cjtw['mode'] = cjtw['mode'].astype('int8')
        # Rename as m
        cjtw = cjtw.rename(columns={'mode': 'm'})
        # Filter to calib params mode
        cjtw = cjtw[cjtw['m'] == calib_params['m']].reset_index(drop=True)
        cjtw = cjtw.drop('m', axis=1)

        commute_pa = target_p.merge(cjtw,
                                    how='left',
                                    left_on=[ia_name],
                                    right_on=[aor_col])

        commute_pa = commute_pa.rename(columns={aor_col: 'p_zone',
                                                aow_col: 'a_zone'})
        commute_pa = commute_pa.drop(ia_name, axis=1)

        # Balance to production totals
        commute_pa['dt'] = commute_pa['productions'] * commute_pa['distribution']

        total_cjtw_productions = commute_pa['dt'].sum()

        commute_pa['p_zone'] = commute_pa['p_zone'].astype('int16')
        commute_pa['a_zone'] = commute_pa['a_zone'].astype('int16')

        # TODO: Log audit variables
        prod_diff = total_commute_productions - total_cjtw_productions

        nup.print_w_toggle('Difference in productions:', str(prod_diff), verbose=verbose)

        # Drop stuff we don't need
        commute_pa = commute_pa.drop(['productions', 'distribution'], axis=1)
        long_pa = commute_pa.copy()

        # Push to wide format
        commute_pa = nup.df_to_np(commute_pa,
                                  v_heading='p_zone',
                                  h_heading='a_zone',
                                  values='dt',
                                  unq_internal_zones=unq_internal_zones,
                                  verbose=verbose)

        commute_pa = pd.DataFrame(commute_pa,
                                  index=unq_internal_zones,
                                  columns=unq_internal_zones)

        # Get costs for trip length curve only
        internal_costs = nup.get_costs(model_lookup_path,
                                       calib_params,
                                       tp=cost_type,
                                       iz_infill=0.5)

        nup.print_w_toggle('Cost lookup returned ' + internal_costs[1], verbose=verbose)
        internal_costs = internal_costs[0].copy()

        # Build a trip length curve
        # TODO: Failing for some reason
        d_bin = nup.build_distribution_bins(internal_costs,
                                            long_pa)

        return commute_pa, d_bin

    def get_distribution_parameters(
            self,
            initial_betas,
            calib_params):
        """
        This function retreives a beta from an initial beta sheet in the input
        folder. It takes the purpose, mode and traget trip lengths.

        Parameters
        ----------
        initial_betas:
            A list of segmented productions for the internal area to be aggregated.
            Will need to have standard NorMITs Synthesiser column names.

        calib_params:
            A dictionary containing calibration parameters for a given distribution
            run.

        Returns
        ----------
        beta:
            24hr productions by zone by mode and purpose. Ready for distribution.

        ttl:
            Target trip length from beta segment.
        """
        beta_subset = initial_betas.copy()

        # Quite lovely dictionary loop to define intial betas.
        # Will throw an error if the betas can't give it an exact beta for a segment.
        # Should be handled by a beta seeding function in the run_gravity_model.
        for index, cp in calib_params.items():
            beta_subset = beta_subset[
                beta_subset[index] == cp
                ].reset_index(drop=True)
        print(beta_subset)
        init_param_a = beta_subset['init_param_a'][0]
        init_param_b = beta_subset['init_param_b'][0]
        ttl = beta_subset['average_trip_length'][0]

        distribution_params = {'init_param_a': init_param_a,
                               'init_param_b': init_param_b,
                               'ttl': ttl}
        return distribution_params

    def translate_distribution(
            self,
            om_dist,
            model_name,
            other_model_name,
            internal_24hr_productions,
            calib_params,
            target_path):

        """
        Translate distribution from one model zoning system to another using
        given lookups
        """

        # Drop segments from distribution, retain p/a & trips
        om_dist = om_dist.reindex(['p_zone', 'a_zone', 'dt'], axis=1)

        # Get original total productions
        om_productions = om_dist['dt'].sum()

        lookup_types = [(other_model_name + '_' + model_name.lower()),
                        (model_name.lower()) + '_' + other_model_name]
        # Check for lookup types, pop weighted and employment weighted
        # TODO: if there isn't one run one - or have this at the outset
        zone_translation_lookups = []
        mzl_dir = os.listdir(target_path)
        for lt in lookup_types:
            ztl = [x for x in mzl_dir if lt in x]
            if len(ztl) > 0:
                zone_translation_lookups.append(ztl)
        # flatten list of lists
        zone_translation_lookups = [x for y in zone_translation_lookups for x in y]
        # Pull out individuals
        zt_pop = [x for x in zone_translation_lookups if 'pop' in x][0]
        zt_emp = [x for x in zone_translation_lookups if 'emp' in x][0]

        zt_pop = pd.read_csv((target_path + '/' + zt_pop))
        zt_emp = pd.read_csv((target_path + '/' + zt_emp))

        # Define index cols for lookups
        li_cols = [(other_model_name + '_zone_id'),
                   (model_name.lower() + '_zone_id'),
                   ('overlap_' + other_model_name + '_split_factor')]

        # Model name first, other model second
        split_factor_col = ('overlap_' + other_model_name + '_split_factor')

        zt_pop = zt_pop.reindex(li_cols, axis=1)
        zt_emp = zt_emp.reindex(li_cols, axis=1)

        # Process to get cleaner matches
        original_model_col = (other_model_name + '_zone_id')
        target_model_col = (model_name.lower() + '_zone_id')

        # Round & drop 0 segments
        zt_pop[split_factor_col] = zt_pop[split_factor_col].round(3)
        zt_pop = zt_pop[zt_pop[split_factor_col] > 0]

        # Correct factors back to 0
        zt_pop_tot = zt_pop.reindex([original_model_col, split_factor_col], axis=1).groupby(
            original_model_col).sum().reset_index()

        # Derive adjustment for factor
        zt_pop_tot['adj_factor'] = 1 / (zt_pop_tot[split_factor_col] / 1)
        zt_pop_tot = zt_pop_tot.drop(split_factor_col, axis=1)
        zt_pop = zt_pop.merge(zt_pop_tot,
                              how='left',
                              on=original_model_col)
        zt_pop[split_factor_col] = zt_pop[split_factor_col] * zt_pop['adj_factor']
        zt_pop = zt_pop.drop('adj_factor', axis=1)

        # Round & drop 0 segments
        zt_emp[split_factor_col] = zt_emp[split_factor_col].round(3)
        zt_emp = zt_emp[zt_emp[split_factor_col] > 0]

        # Correct factors back to 0
        zt_emp_tot = zt_emp.reindex([original_model_col,
                                     split_factor_col],
                                    axis=1).groupby(
            original_model_col).sum().reset_index()
        # Derive adjustment for factor
        zt_emp_tot['adj_factor'] = 1 / (zt_emp_tot[split_factor_col] / 1)
        zt_emp_tot = zt_emp_tot.drop(split_factor_col, axis=1)
        zt_emp = zt_emp.merge(zt_emp_tot,
                              how='left',
                              on=original_model_col)
        zt_emp[split_factor_col] = zt_emp[split_factor_col] * zt_emp['adj_factor']
        zt_emp = zt_emp.drop('adj_factor', axis=1)
        # zone translations defined

        # Rename destination zone names to join w/o duplication
        zt_pop = zt_pop.rename(
            columns={(other_model_name + '_zone_id'): 'p_zone'})
        zt_emp = zt_emp.rename(
            columns={(other_model_name + '_zone_id'): 'a_zone'})

        # Transform model demand to current model zones
        # Bring in population split for productions
        om_dist = om_dist.merge(zt_pop,
                                how='left',
                                on='p_zone')
        # Multiply out
        om_dist['dt'] = om_dist['dt'] * om_dist[('overlap_' + other_model_name + '_split_factor')]
        om_dist = om_dist.drop(
            ['p_zone',
             'overlap_' + other_model_name + '_split_factor'], axis=1)
        om_dist = om_dist.rename(
            columns={(model_name.lower() + '_zone_id'): 'p_zone'})
        # Group and sum
        group_cols = ['p_zone', 'a_zone']
        sum_cols = group_cols.copy()
        sum_cols.append('dt')

        om_dist = om_dist.reindex(
            sum_cols, axis=1).groupby(
            group_cols).sum().reset_index()

        # Bring in employment split for attractions
        om_dist = om_dist.merge(zt_emp,
                                how='left',
                                on='a_zone')
        # Multiply out
        om_dist['dt'] = om_dist['dt'] * om_dist[('overlap_' + other_model_name + '_split_factor')]
        om_dist = om_dist.drop(
            ['a_zone',
             'overlap_' + other_model_name + '_split_factor'], axis=1)
        om_dist = om_dist.rename(
            columns={(model_name.lower() + '_zone_id'): 'a_zone'})
        # Group and sum
        om_dist = om_dist.reindex(
            sum_cols, axis=1).groupby(
            group_cols).sum().reset_index()

        # Audit total
        if om_productions.round(0) == om_dist['dt'].sum().round(0):
            print('Balanced well')
        else:
            print('Balance off')  # TODO: Bit more

        # Reduce to zonal factors to apply new productions to
        om_dist_p_totals = om_dist.reindex(['p_zone', 'dt'], axis=1).groupby('p_zone').sum().reset_index()
        om_dist_p_totals = om_dist_p_totals.rename(columns={'dt': 'dt_total'})

        om_dist = om_dist.merge(om_dist_p_totals,
                                how='left',
                                on='p_zone')
        # Derive p/a share by p
        om_dist['p_a_share'] = om_dist['dt'] / om_dist['dt_total']
        om_dist = om_dist.drop(['dt', 'dt_total'], axis=1)

        # Get target productions
        target_p = nup.filter_distribution_p(internal_24hr_productions,
                                             model_name.lower() + '_zone_id',
                                             calib_params,
                                             round_val=3)

        original_p = target_p['productions'].sum()
        target_p = target_p.rename(columns={target_model_col: 'p_zone'})

        # Join on distribution
        target_p = target_p.merge(om_dist,
                                  how='left',
                                  on='p_zone')
        target_dist = target_p.copy()
        target_dist['dt'] = (target_dist['productions'] *
                             target_dist['p_a_share'])
        target_dist = target_dist.drop(['productions', 'p_a_share'], axis=1)

        # Correct for dropped trips as required
        correction_factor = 1 / (target_dist['dt'].sum() / original_p)
        if correction_factor > 1.1 or correction_factor < 0.9:
            Warning('Massive correction required for balance, check inputs')
        target_dist['dt'] = target_dist['dt'] * correction_factor

        # Audit total
        if original_p.round(0) == target_dist['dt'].sum().round(0):
            print('Balanced well')
        else:
            print('Balance off')  # TODO: Bit more

        # Reapply segments
        col_ph = ['p_zone', 'a_zone']
        for key, value in calib_params.items():
            col_ph.append(key)
            print('Reappending segment values for ' + key)
            target_dist[key] = value
        col_ph.append('dt')

        # Final order preserving reindex
        target_dist = target_dist.reindex(col_ph, axis=1)

        return (target_dist)

    def pass_to_intrazonal(self,
                           internal_24hr_productions,
                           ia_name,
                           calib_params,
                           verbose=True):

        """
        Function to pass distributions straight to intrazonal as required.
        For active modes and PT until MSOA distribution in March-20
        """
        # Get unique internal zones in a smart way
        min_zone = min(internal_24hr_productions[ia_name])
        max_zone = max(internal_24hr_productions[ia_name])
        unq_internal_zones = [i for i in range(min_zone, max_zone + 1)]

        target_p = nup.filter_distribution_p(internal_24hr_productions,
                                             ia_name,
                                             calib_params,
                                             round_val=3,
                                             verbose=verbose)[0]
        intra_dist = target_p
        intra_dist['p_zone'] = intra_dist[ia_name]
        intra_dist['a_zone'] = intra_dist[ia_name]
        intra_dist = intra_dist.drop(ia_name,
                                     axis=1)
        intra_dist = intra_dist.rename(columns={'productions': 'dt'})

        # Re-append values
        reindex_cols = ['p_zone', 'a_zone']
        for key, value in calib_params.items():
            intra_dist[key] = value
            reindex_cols.append(key)

        # Reindex cols to match standard
        reindex_cols.append('dt')
        intra_dist = intra_dist.reindex(
            reindex_cols, axis=1).reset_index(drop=True)

        # Push to wide format
        intra_dist = nup.df_to_np(intra_dist,
                                  v_heading='p_zone',
                                  h_heading='a_zone',
                                  values='dt',
                                  unq_internal_zones=unq_internal_zones,
                                  verbose=verbose)

        intra_dist = pd.DataFrame(intra_dist,
                                  index=unq_internal_zones,
                                  columns=unq_internal_zones)

        return intra_dist

    def run_separation(self,
                       initial_betas,
                       available_dists):

        """
        Function to separate initial betas based on the type of distribution required.

        null_dists = Distributions to be ignored (usually cjtw)
        intra_zonal_dists = Combos to be passed straight to intra zonal and exported
        zone_conversion_dists = Combos to be translated from another zoning system
        initial_betas = actual distribution betas

        Parameters
        ----------
        initial_betas:
            Dataframe containing segmentations, initial parameters and distribution type

        available_dists:

        """
        # Get len at the start
        beta_length_start = len(initial_betas.iloc[:, 0])

        null_dists = initial_betas[
            initial_betas['distribution'] == 'none'
            ].copy().reset_index(drop=True)

        cjtw_dists = initial_betas[
            initial_betas['distribution'] == 'cjtw'
            ].copy().reset_index(drop=True)

        intra_zonal_dists = initial_betas[
            initial_betas['distribution'] == 'intra'
            ].copy().reset_index(drop=True)

        translated_dists = initial_betas[
            initial_betas['distribution'].isin(available_dists)
        ].copy().reset_index(drop=True)

        initial_betas = initial_betas[
            initial_betas['distribution'] == 'synthetic'
            ].reset_index(drop=True)

        beta_length_end = sum([len(null_dists.iloc[:, 0]),
                               len(cjtw_dists.iloc[:0]),
                               len(intra_zonal_dists.iloc[:, 0]),
                               len(translated_dists.iloc[:, 0]),
                               len(initial_betas.iloc[:, 0])])
        if beta_length_start == beta_length_end:
            print('All betas accounted for')
        else:
            Warning('Some betas dropped')

        print("inital betas", initial_betas,
              "transalated dists", translated_dists,
              "intra_zonal_dists", intra_zonal_dists,
              "cjtw_dists", cjtw_dists,
              "null_dists", null_dists)
        return (initial_betas,
                translated_dists,
                intra_zonal_dists,
                cjtw_dists,
                null_dists)

    def fetch_other_model_distribution(other_model_name,
                                       file_drive,
                                       available_dists,
                                       calib_params,
                                       distribution_segments,
                                       t_iteration=None,
                                       verbose=True):

        """
        Function to smartly path into another distribution, as specified in
        a set of target betas.
        Returns a distribution in long format (internal 24hr PA)
        """
        # TODO: Add ability to take dist from fusion output!

        l_available_dists = [x.lower() for x in available_dists]

        t_import_source = other_model_name
        t_import_path = os.path.join(file_drive,
                                     'NorMITs Synthesiser',
                                     available_dists[l_available_dists.index(t_import_source)])

        # Find largest iter
        if t_iteration is None:
            t_import_dir = os.listdir(t_import_path)
            t_iter_list = []
            for item in t_import_dir:
                if len(item) < 7 and item.startswith('iter'):
                    if item == 'iterx':
                        break
                    else:
                        t_iter_list.append(item)
                    # Remove iterx
            t_iteration = max(t_iter_list)
        else:
            nup.print_w_toggle('Checking for ' + str(t_iteration), verbose=verbose)

        t_import_path = os.path.join(t_import_path,
                                     t_iteration,
                                     'Distribution Outputs',
                                     '24hr PA Distributions')

        # Try and get a matching distribution from the folder
        # TODO: Update to lookup by segment, or at least try
        t_import_dir = os.listdir(t_import_path)

        cal_mode = ('mode_' + str(calib_params['m']))
        cal_purpose = ('purpose_' + str(calib_params['p']))
        t_import_dist = [x for x in t_import_dir if cal_mode in x]
        t_import_dist = [x for x in t_import_dist if cal_purpose in x]

        # If more than 1, try segments!
        # Drop mode and purpose from segments
        other_segments = distribution_segments.copy()
        other_segments.remove('m')
        other_segments.remove('p')

        if len(t_import_dist) > 1:
            t_import_dist_seg = t_import_dist
            # Check if we can make a match to segmented data
            for segment in other_segments:
                cal_seg = (segment + '_' + str(calib_params[segment]))
                t_import_dist_seg = [x for x in t_import_dist_seg if cal_seg in x]
            # If yes use that going forward
            if len(t_import_dist_seg) == 1:
                t_import_dist = t_import_dist_seg
                # TODO: Smarter append to ignored segment list
                # ignored_segs = None
        else:
            nup.print_w_toggle('Ignored segs on reimport ', other_segments, verbose=verbose)
            # ignored_segs = other_segments.copy()

        dist_ph = []
        for item in t_import_dist:
            temp_dist = pd.read_csv(os.path.join(t_import_path, item))
            temp_dist = nup.optimise_data_types(temp_dist, verbose=verbose)
            dist_ph.append(temp_dist)
            del (temp_dist)

        t_dist = pd.concat(dist_ph, sort=True)

        # Used to return ignored segs but I don't want it to anymore
        return t_import_dist, t_dist

    def _synth_dist_worker(self,
                           dist_index,
                           synthetic_dists,
                           segmentation,
                           distribution_segments,
                           dist_function,
                           tlb_area,
                           trip_origin,
                           target_trip_lengths,
                           ia_name,
                           productions,
                           attractions,
                           cost_type,
                           furness_loops,
                           fitting_loops,
                           i_paths,
                           o_paths,
                           iz_cost_infill,
                           verbose=True,
                           ) -> None:
        """
        Distributes the synthetic distributions, producing 24hr PA Matrices

        Encapsulates the inner workings of the synthetic distribution loop from
        run_distribution_model(). Useful for use in multi-threaded environments.

        Parameters
        ----------
        dist_index:
            Index corresponding to each synthetic distribution segmentation.

        synthetic_dists:
            Contains all the synthetic distribution segmentations to be run.

        segmentation:
            Target segmentation, takes 'ntem' or 'tfn' (not used in code).

        distribution_segments:
            Segments to distribute to at 24hr PA.

        dist_function:
            distribution method for gravity model - takes 'tanner' or 'ln'.

        tlb_area:
            The area that is to be considered for trip length bands.

        target_trip_lengths:
        ia_name:
            Contains the column name that has zone numbers.

        productions:
            Internal production data.

        attractions:
            Internal attraction data.

        cost_type:
            The cost type used for trip length curve. Usually '24hr' for Noham and 'tp' for Norms.

        furness_loops:
            Number of inner loop iterations to do before abandoning.

        fitting_loops:
            Number of outer loop iterations to complete before abandoning.

        i_paths:
            Dictionary containing paths to import, model lookups, production and attraction files.

        o_paths:
            Dictionary containing various export paths are required.

        iz_cost_infill:
            Intra zonal cost infill.

        verbose:
            Indicates whether to print a log of the process to the terminal.
            Useful to set verbose=False when using multi-threaded loops.
            Defaults to True.
        """
        # get all segments and define parameters for distribution
        calib_params = {}
        for ds in distribution_segments:
            calib_params.update({ds: target_trip_lengths[ds][dist_index]})

        # Get initial alpha & beta from distribution parameters
        distribution_params = self.get_distribution_parameters(
            synthetic_dists, calib_params)

        # Print a message telling the user which matrix is being worked on
        dist_name = nup.generate_distribution_name(calib_params)
        print("INFO: Creating the synthetic distribution for %s..." % dist_name)

        # Unpack params into dict
        init_param_a = distribution_params['init_param_a']
        init_param_b = distribution_params['init_param_b']
        print(distribution_params)

        del distribution_params

        # Work out trip length band path from given params
        # Only want standard segments at this point
        tlb_folder = os.path.join(i_paths['imports'],
                                  'trip_length_bands',
                                  tlb_area,
                                  'standard_segments')

        # Get trip length bands
        # TODO (BT): Get this to happen a bit more naturally
        # Ignore ca unless we are doing rail (m6)
        temp_calib_params = calib_params.copy()
        if calib_params['m'] != 6:
            temp_calib_params.pop('ca', None)
        tlb = nup.get_trip_length_bands(tlb_folder, temp_calib_params, segmentation,
                                        trip_origin=trip_origin, replace_nan=True)

        calib_params.update({'tlb': tlb})

        # Loop over the distributions until beta gives:
        # 1. a decent average trip length by band
        # 2. decent proportions within the segments

        # TODO: Filter should be done outside of function and passed as np vector, still inside atm
        #  - this applies to intra & cjtw too
        hb_distribution = gm.run_gravity_model(
            ia_name,
            calib_params,
            init_param_a,
            init_param_b,
            productions,
            attractions,
            model_lookup_path=i_paths['lookups'],
            tlb=tlb,
            dist_log_path=o_paths['reports'],
            dist_log_fname=trip_origin + '_internal_distribution',
            dist_function=dist_function,
            cost_type=cost_type,
            apply_k_factoring=True,
            furness_loops=furness_loops,
            fitting_loops=fitting_loops,
            bs_con_target=.95,
            target_r_gap=1,
            rounding_factor=3,
            iz_cost_infill=iz_cost_infill,
            verbose=verbose
        )

        # Export distribution
        dist_path = os.path.join(o_paths['summaries'],
                                 trip_origin + '_synthetic')
        dist_path = nup.build_path(dist_path, calib_params)
        hb_distribution[0].to_csv(dist_path, index=False)

        # Export trip length bins
        bin_path = os.path.join(o_paths['tld'],
                                trip_origin + '_synthetic_bin')
        bin_path = nup.build_path(bin_path, calib_params)
        hb_distribution[1].to_csv(bin_path, index=False)

        del hb_distribution

    def _trans_dist_worker(
            self,
            dist_index,
            translated_dists,
            available_dists,
            model_name,
            distribution_segments,
            trip_origin,
            internal_24hr_productions,
            i_paths,
            o_paths,
            verbose=True):
        """
        Distributes the translated distributions, producing 24hr PA Matrices

        Encapsulates the inner workings of the translated distribution loop from
        run_distribution_model(). Useful for use in multi-threaded environments
        """
        # TODO: make sure verbose=False it actually suppresses all prints
        calib_params = {}
        for ds in distribution_segments:
            calib_params.update({ds: translated_dists[ds][dist_index]})

        # Print a message telling the user which matrix is being worked on
        dist_name = nup.generate_distribution_name(calib_params)
        print("INFO: Creating the synthetic distribution for %s..." % dist_name)

        # Get distribution from import source
        # TODO: tighten up calls to other dists
        other_model_name = translated_dists['distribution'][dist_index]
        omd = self.fetch_other_model_distribution(
            other_model_name,
            available_dists,
            calib_params,
            distribution_segments,
            t_iteration='iter4',
            verbose=verbose)

        om_name = omd[0]
        nup.print_w_toggle('Using ' + str(om_name), verbose=verbose)
        om_dist = omd[1]
        # om_ignored_segs = omd[2]
        # print('This distribution ignoring ' + str(om_ignored_segs) +
        #       ' this may mean distribution ignores segment costs')

        del omd
        source_trips = om_dist['dt'].sum()
        nup.print_w_toggle(
            'Source trips: ' + str(source_trips), verbose=verbose)

        t_distribution = self.translate_distribution(
            om_dist,
            model_name,
            other_model_name,
            internal_24hr_productions,
            calib_params,
            target_path=i_paths['lookups'])

        target_trips = t_distribution['dt'].sum()
        nup.print_w_toggle('Target trips: ' + str(target_trips), verbose=verbose)

        # Export translated distribution
        t_dist_path = os.path.join(o_paths['summaries'],
                                   trip_origin + '_synthetic')
        t_dist_path = nup.build_path(t_dist_path, calib_params)

        t_distribution.to_csv(t_dist_path, index=False)

    def _iz_dist_worker(
            self,
            dist_index,
            intra_zonal_dists,
            distribution_segments,
            trip_origin,
            internal_24hr_productions,
            ia_name,
            o_paths,
            verbose=True):
        """
        Distributes the intra_zonal distributions, producing 24hr PA Matrices

        Encapsulates the inner workings of the intra_zonal distribution loop from
        run_distribution_model(). Useful for use in multi-threaded environments
        """
        calib_params = {}
        for ds in distribution_segments:
            calib_params.update({ds: intra_zonal_dists[ds][dist_index]})

        # Print a message telling the user which matrix is being worked on
        dist_name = nup.generate_distribution_name(calib_params)
        print("INFO: Creating the intra_zonal distribution for %s..." % dist_name)

        iz_distribution = self.pass_to_intrazonal(
            internal_24hr_productions,
            ia_name,
            calib_params,
            verbose=verbose)

        # Export translated distribution
        iz_dist_path = os.path.join(o_paths['summaries'],
                                    trip_origin + '_synthetic')
        iz_dist_path = nup.build_path(iz_dist_path, calib_params)

        iz_distribution.to_csv(iz_dist_path)

    def _cjtw_dist_worker(
            self,
            dist_index,
            cjtw_dists,
            model_name,
            distribution_segments,
            trip_origin,
            internal_24hr_productions,
            cost_type,
            ia_name,
            i_paths,
            o_paths,
            verbose=True):
        """
        Distributes the cjtw distributions, producing 24hr PA Matrices

        Encapsulates the inner workings of the cjtw distribution loop from
        run_distribution_model(). Useful for use in multi-threaded environments
        """

        # TODO: Get to work in same way as others
        calib_params = {}
        for ds in distribution_segments:
            calib_params.update({ds: cjtw_dists[ds][dist_index]})

        # Print a message telling the user which matrix is being worked on
        dist_name = nup.generate_distribution_name(calib_params)
        print("INFO: Creating the synthetic distribution for %s..." % dist_name)

        # Doesn't look like this is used?
        # tlb_folder = os.path.join(i_paths['imports'],
        #                           'trip_length_bands',
        #                           tlb_area,
        #                           'standard_segments')
        # Get trip length bands
        # tlb = nup.get_trip_length_bands(tlb_folder,
        #                                 calib_params,
        #                                 segmentation,
        #                                 trip_origin = trip_origin)

        if cost_type != '24hr':
            print('WARNING: Costs are not 24hr, trip length band audit may not work.')

        cjtw_distribution = self.distribute_cjtw(
            internal_24hr_productions,
            i_paths['lookups'],
            ia_name,
            model_name,
            calib_params,
            cost_type=cost_type,
            subset=None,
            verbose=verbose)

        # Export 24hr distribution
        # Generate distribution name
        c_dist_path = os.path.join(o_paths['summaries'],
                                   trip_origin + '_synthetic')
        c_dist_path = nup.build_path(c_dist_path, calib_params)

        # Generate trip length bin name
        c_bin_path = os.path.join(o_paths['tld'],
                                  trip_origin + '_synthetic_bin')
        c_bin_path = nup.build_path(c_bin_path, calib_params)

        # Export distribution
        cjtw_distribution[0].to_csv(c_dist_path, index=True)
        # Export trip length bins
        cjtw_distribution[1].to_csv(c_bin_path, index=False)

    def run_distribution_model(self,
                               file_drive: nd.PathLike,
                               model_name: str,
                               iteration: str,
                               tlb_area: str = 'north',
                               segmentation: str = 'tfn',
                               distribution_segments: list = ['p', 'm'],
                               dist_function: str = 'tanner',
                               trip_origin: str = 'hb',
                               cost_type: str = '24hr',
                               furness_loops: int = 1999,
                               fitting_loops: int = 100,
                               iz_cost_infill: float = .5,
                               export_modes=[3],
                               mp_threads: int = -1,
                               verbose: bool = True,
                               ) -> None:

        """
        This function runs the distribution of productions and attractions to a
        24hr PA matrix.

        Parameters
        ----------
        file_drive:
            Name of root drive to do work on.

        model_name:
            Name of the model. Whether it is noham or norms.

        iteration:
            Iteration of current run. This will be in output names and also the
            target folder.

        tlb_area:
            The area that is to be considered for trip length bands.

        segmentation:
            Target segmentation, takes 'ntem' or 'tfn' (not used in code).

        distribution_segments:
            Segments to distribute to at 24hr PA.

        dist_function:
            distribution method for gravity model - takes 'tanner' or 'ln'.

        trip_origin:
            Whether it is home or non home based trip.

        cost_type:
            The cost type used for trip length curve. Usually '24hr' for Noham and 'tp' for Norms.

        furness_loops:
            Number of inner loop iterations to do before abandoning.

        fitting_loops:
            Number of outer loop iterations to complete before abandoning.

        iz_cost_infill:
            Intra zonal cost infill.

        export_modes:
            Not used anymore in code. Need checking

        verbose:
            Indicates whether to print a log of the process to the terminal.
            Useful to set verbose=False when using multi-threaded loops.
            Defaults to True.

        mp_threads:
            The number of threads to use when multiprocessing.
            Set mp_threads=1 to not use multiple threads.
            Set mp_threads=0 to not use multi-threaded code at all.
            Defaults to 0.

        Returns
        ----------
        None
        """
        # Argument validation
        assert (mp_threads >= -2), ("In distribution_model.run_distribution_model(), " +
                                    "mp_threads must be -1 or greater.\n" +
                                    "Got %s." % str(mp_threads))
        if mp_threads < 0:
            mp_threads = os.cpu_count() + mp_threads

        # Path to root, stop any folder overwrites

        # Define paths for import and export
        paths = self.path_config(file_drive=file_drive,
                                 model_name=model_name,
                                 iteration=iteration,
                                 trip_origin=trip_origin)
        i_paths = paths[0]
        o_paths = paths[1]
        del paths

        # Define list of available distributions (for translation)
        # TODO: filter out fluff, lower case
        # TODO: Push to run script
        available_dists = os.listdir(file_drive)

        # Paths returns:
        # (import_folder_path, model_lookup_path,
        # p_import_path, a_import_path, export_list)

        print('working folder:' + os.getcwd())
        print('importing global params from:' + i_paths['imports'])
        print('lookups from:' + i_paths['lookups'])
        print('productions from:' + i_paths['production_import'])
        print('attractions from:' + i_paths['attraction_import'])

        productions = pd.read_csv(i_paths['production_import'])
        attractions = pd.read_csv(i_paths['attraction_import'])

        ia_areas = nup.define_internal_external_areas(i_paths['lookups'])
        internal_area = ia_areas[0]
        ia_name = list(internal_area)[0]
        print("ia_name", ia_name)
        external_area = ia_areas[1]

        ### TODO: Distribution model is an object and this is the end of the _init_
        ### End of production core, ready for all homebased runs.

        # TODO: Build the init_params, if there aren't any

        # Get initial betas
        init_params = nup.get_init_params(i_paths['lookups'],
                                          distribution_type=trip_origin,
                                          model_name=model_name,
                                          mode_subset=None,
                                          purpose_subset=None)

        # null_dists = Distributions to be ignored (usually cjtw)
        # intra_zonal_dists = Combos to be passed straight to intra zonal and exported
        # zone_conversion_dists = Combos to be translated from another zoning system
        # init_params = actual distribution alphas & betas

        segmented_params = self.run_separation(init_params,
                                               # lower dist list
                                               [x.lower() for x in available_dists])

        synthetic_dists = segmented_params[0]
        translated_dists = segmented_params[1]  # TODO: Specify iteration
        intra_zonal_dists = segmented_params[2]
        cjtw_dists = segmented_params[3]
        null_dists = segmented_params[4]

        # Get target trip length
        ttl_cols = distribution_segments.copy()
        ttl_cols.append('average_trip_length')
        print("ttl_cols", ttl_cols)

        # Target trip lengths here, derived straight from beta file
        target_trip_lengths = synthetic_dists.reindex(ttl_cols, axis=1).reset_index(drop=True)

        print("target_trip_lengths", target_trip_lengths)

        # Let the user know if threads are being used or not
        if mp_threads == 0:
            print("INFO: mp_threads set to 0, running as basic loops.")
        elif mp_threads == 1:
            print("INFO: mp_threads set to 1, using a single thread.")
        else:
            print("INFO: mp_threads set to 2 or greater, using multiple threads.")

        # Iterate over distributions
        print("-" * 20, " Start Synthetic Distributions ", "-" * 20)
        # Build a dictionary of unchanging keyword arguments for each function call
        unchanging_kwargs = {'synthetic_dists': synthetic_dists,
                             'segmentation': segmentation,
                             'distribution_segments': distribution_segments,
                             'dist_function': dist_function,
                             'tlb_area': tlb_area,
                             'trip_origin': trip_origin,
                             'target_trip_lengths': target_trip_lengths,
                             'ia_name': ia_name,
                             'productions': productions,
                             'attractions': attractions,
                             'cost_type': cost_type,
                             'furness_loops': furness_loops,
                             'fitting_loops': fitting_loops,
                             'i_paths': i_paths,
                             'o_paths': o_paths,
                             'iz_cost_infill': iz_cost_infill,
                             'verbose': verbose}

        if mp_threads == 0:
            for i in target_trip_lengths.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                print('Running ' + str(i))
                self._synth_dist_worker(**kwargs)
        else:
            # Build a list of kwargs for each function call
            kwargs_list = list()
            for i in target_trip_lengths.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                kwargs_list.append(kwargs)

            # Call using multiple threads
            mp.process_pool_wrapper(
                self._synth_dist_worker,
                kwargs=kwargs_list,
                process_count=mp_threads,
            )

        del unchanging_kwargs
        print("-" * 20, " End Synthetic Distributions ", "-" * 20)

        print("-" * 20, " Start Translated Distributions ", "-" * 20)

        # Build a dictionary of unchanging keyword arguments for each function call
        unchanging_kwargs = {'translated_dists': translated_dists,
                             'available_dists': available_dists,
                             'model_name': model_name,
                             'distribution_segments': distribution_segments,
                             'trip_origin': trip_origin,
                             'internal_24hr_productions': productions,
                             'i_paths': i_paths,
                             'o_paths': o_paths,
                             'file_drive': file_drive,
                             'verbose': verbose}

        if mp_threads == 0:
            for i in translated_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                self._trans_dist_worker(**kwargs)
        else:
            # Build a list of kwargs for each function call
            kwargs_list = list()
            for i in translated_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                kwargs_list.append(kwargs)

            # Call using multiple threads
            mp.process_pool_wrapper(self._trans_dist_worker,
                                    kwargs=kwargs_list,
                                    process_count=mp_threads)
        del unchanging_kwargs
        print("-" * 20, " End Translated Distributions ", "-" * 20)

        print("-" * 20, " Start Intra-Zonal Distributions ", "-" * 20)
        # Build a dictionary of unchanging keyword arguments for each function call
        unchanging_kwargs = {'intra_zonal_dists': intra_zonal_dists,
                             'distribution_segments': distribution_segments,
                             'trip_origin': trip_origin,
                             'internal_24hr_productions': productions,
                             'ia_name': ia_name,
                             'o_paths': o_paths,
                             'verbose': verbose}

        if mp_threads == 0:
            for i in intra_zonal_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                self._iz_dist_worker(**kwargs)
        else:
            # Build a list of kwargs for each function call
            kwargs_list = list()
            for i in intra_zonal_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                kwargs_list.append(kwargs)

            # Call using multiple threads
            mp.process_pool_wrapper(self._iz_dist_worker,
                                    kwargs=kwargs_list,
                                    process_count=mp_threads)
        del unchanging_kwargs
        print("-" * 20, " End Intra-Zonal Distributions ", "-" * 20)

        print("-" * 20, " Start CJTW Distributions ", "-" * 20)
        # Build a dictionary of unchanging keyword arguments for each function call
        unchanging_kwargs = {'cjtw_dists': cjtw_dists,
                             'model_name': model_name,
                             'distribution_segments': distribution_segments,
                             'trip_origin': trip_origin,
                             'internal_24hr_productions': productions,
                             'cost_type': cost_type,
                             'ia_name': ia_name,
                             'i_paths': i_paths,
                             'o_paths': o_paths,
                             'verbose': verbose}

        if mp_threads == 0:
            for i in cjtw_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                self._cjtw_dist_worker(**kwargs)
        else:
            # Build a list of kwargs for each function call
            kwargs_list = list()
            for i in cjtw_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                kwargs_list.append(kwargs)

            # Call using multiple threads
            mp.process_pool_wrapper(self._cjtw_dist_worker,
                                    kwargs=kwargs_list,
                                    process_count=mp_threads)
        del unchanging_kwargs
        print("-" * 20, " End CJTW Distributions ", "-" * 20)
        print('Done!')
