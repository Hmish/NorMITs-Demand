# -*- coding: utf-8 -*-
"""
Created on Tue Jun 18 12:17:07 2019

Script to distribute the Commute productions generated by the production model
using the census journey to work data reformatted to model dimensions

@author: cruella
"""

import pandas as pd # most of the heavy lifting
import os # File ops

import normits_demand.models.tms as tms

from normits_demand.concurrency import multiprocessing as mp
from normits_demand.distribution import gravity_model as gm # For distribution functions
from normits_demand.utils import utils as nup # Folder management, reindexing, optimisation


class DistributionModel(tms.TMSPathing):
    pass

    def distribute_cjtw(
            self,
            internal_24hr_productions,
                        model_lookup_path,
                        ia_name,
                        model_name,
                        calib_params,
                        cost_type = '24hr',
                        subset=None,
                        verbose=True):
        """
        This distributes commute productions by census journey to work distributions
        using zone to zone movements as a factor.

        Parameters
        ----------
        internal_24hr_productions:
            Commute only productions, should be pre-filtered before coming into
            function.

        ia_name:
            The name of the internal area of the model in use. Required for
            ensuring the right columns come through.

        model_name:
            Name of model. From global definition

        distribution_segments:
            Segments to distribute to. From distribution model definition.

        model_lookup_path:
            A model folder to pass to get_cjtw to find a census journey to work
            distribution.

        subset:
            Takes a vector of model zones to filter by. Mostly for test model runs.

        verbose:
            Indicates whether to print a log of the process to the terminal.
            Useful to set verbose=False when using multi-threaded loops.
            Defaults to True.

        Returns
        ----------
        [0] commute_pa:
            A 24hr commute matrix distributed by census journey to work, internal
            only, wide format.

        [1] d_bin:
            Trip length distribution bins (km) for cjtw distribution
        """

        # Get unique internal zones in a smart way
        min_zone = min(internal_24hr_productions['p_zone'])
        max_zone = max(internal_24hr_productions['p_zone'])
        unq_internal_zones = [i for i in range(min_zone,max_zone+1)]

        # Filter to calib params - should be p1 only
        target_p = nup.filter_pa_cols(internal_24hr_productions,
                                      'p_zone',
                                      calib_params,
                                      round_val=3,
                                      verbose=verbose)[0]
        target_p = target_p.rename(
                columns={list(target_p)[-1]: 'productions'})

        target_p = target_p.rename(columns={'p_zone': ia_name})

        # Get productions before
        total_commute_productions = target_p['productions'].sum()

        # Get census journey to work
        cjtw = nup.get_cjtw(model_lookup_path,
                            model_name,
                            subset=subset)

        # Get area of residence col name
        aor_col = list(cjtw)[0]
        # Get area of workplace col name
        aow_col = list(cjtw)[1]
        # Change mode desc col to int for join
        # TODO: I would like this type change to be done in the function call
        cjtw['mode'] = cjtw['mode'].astype('int8')
        # Rename as m
        cjtw = cjtw.rename(columns={'mode': 'm'})
        # Filter to calib params mode
        cjtw = cjtw[cjtw['m']==calib_params['m']].reset_index(drop=True)
        cjtw = cjtw.drop('m', axis=1)

        commute_pa = target_p.merge(cjtw,
                                    how='left',
                                    left_on=[ia_name],
                                    right_on=[aor_col])

        commute_pa = commute_pa.rename(columns={aor_col: 'p_zone',
                                                aow_col: 'a_zone'})
        commute_pa = commute_pa.drop(ia_name, axis=1)

        # Balance to production totals
        commute_pa['dt'] = commute_pa['productions'] * commute_pa['distribution']

        total_cjtw_productions = commute_pa['dt'].sum()

        commute_pa['p_zone'] = commute_pa['p_zone'].astype('int16')
        commute_pa['a_zone'] = commute_pa['a_zone'].astype('int16')

        # TODO: Log audit variables
        prod_diff = total_commute_productions - total_cjtw_productions

        nup.print_w_toggle('Difference in productions:', str(prod_diff), verbose=verbose)

        # Drop stuff we don't need
        commute_pa = commute_pa.drop(['productions', 'distribution'], axis=1)
        long_pa = commute_pa.copy()

        # Push to wide format
        commute_pa = nup.df_to_np(commute_pa,
                                  v_heading='p_zone',
                                  h_heading='a_zone',
                                  values='dt',
                                  unq_internal_zones=unq_internal_zones,
                                  verbose=verbose)

        commute_pa = pd.DataFrame(commute_pa,
                                  index=unq_internal_zones,
                                  columns=unq_internal_zones)

        # Get costs for trip length curve only
        internal_costs = nup.get_costs(model_lookup_path,
                                       calib_params,
                                       tp=cost_type,
                                       iz_infill=0.5)

        nup.print_w_toggle('Cost lookup returned ' + internal_costs[1], verbose=verbose)
        internal_costs = internal_costs[0].copy()

        # Build a trip length curve
        # TODO: Failing for some reason
        d_bin = nup.build_distribution_bins(internal_costs,
                                            long_pa,
                                            verbose=verbose)

        return commute_pa, d_bin

    def get_distribution_parameters(
            self,
            initial_betas,
            calib_params):
        """
        This function retreives a beta from an initial beta sheet in the input
        folder. It takes the purpose, mode and traget trip lengths.

        Parameters
        ----------
        initial_betas:
            A list of segmented productions for the internal area to be aggregated.
            Will need to have standard NorMITs Synthesiser column names.

        calib_params:
            A dictionary containing calibration parameters for a given distribution
            run.

        Returns
        ----------
        beta:
            24hr productions by zone by mode and purpose. Ready for distribution.

        ttl:
            Target trip length from beta segment.
        """
        beta_subset = initial_betas.copy()

        # Quite lovely dictionary loop to define intial betas.
        # Will throw an error if the betas can't give it an exact beta for a segment.
        # Should be handled by a beta seeding function in the run_gravity_model.
        for index, cp in calib_params.items():
            beta_subset = beta_subset[
                    beta_subset[index] == cp
            ].reset_index(drop=True)

        init_param_a = beta_subset['init_param_a'][0]
        init_param_b = beta_subset['init_param_b'][0]
        ttl = beta_subset['average_trip_length'][0]

        distribution_params = {'init_param_a':init_param_a,
                               'init_param_b':init_param_b,
                               'ttl':ttl}
        return distribution_params

    def translate_distribution(
        self,
        om_dist,
        model_name,
        other_model_name,
        internal_24hr_productions,
        calib_params,
        target_path):

        """
        Translate distribution from one model zoning system to another using
        given lookups
        """

        # Drop segments from distribution, retain p/a & trips
        om_dist = om_dist.reindex(['p_zone', 'a_zone', 'dt'], axis=1)

        # Get original total productions
        om_productions = om_dist['dt'].sum()

        lookup_types = [(other_model_name + '_' + model_name.lower()),
                        (model_name.lower()) + '_' + other_model_name]
        # Check for lookup types, pop weighted and employment weighted
        # TODO: if there isn't one run one - or have this at the outset
        zone_translation_lookups = []
        mzl_dir = os.listdir(target_path)
        for lt in lookup_types:
            ztl = [x for x in mzl_dir if lt in x]
            if len(ztl) > 0:
                zone_translation_lookups.append(ztl)
        # flatten list of lists
        zone_translation_lookups = [x for y in zone_translation_lookups for x in y]
        # Pull out individuals
        zt_pop = [x for x in zone_translation_lookups if 'pop' in x][0]
        zt_emp = [x for x in zone_translation_lookups if 'emp' in x][0]

        zt_pop = pd.read_csv((target_path + '/' + zt_pop))
        zt_emp = pd.read_csv((target_path + '/' + zt_emp))

        # Define index cols for lookups
        li_cols = [(other_model_name + '_zone_id'),
                   (model_name.lower() + '_zone_id'),
                   ('overlap_' + other_model_name + '_split_factor')]

        # Model name first, other model second
        split_factor_col = ('overlap_' + other_model_name + '_split_factor')

        zt_pop = zt_pop.reindex(li_cols, axis=1)
        zt_emp = zt_emp.reindex(li_cols, axis=1)

        # Process to get cleaner matches
        original_model_col = (other_model_name + '_zone_id')
        target_model_col = (model_name.lower() + '_zone_id')

        # Round & drop 0 segments
        zt_pop[split_factor_col] = zt_pop[split_factor_col].round(3)
        zt_pop = zt_pop[zt_pop[split_factor_col]>0]

        # Correct factors back to 0
        zt_pop_tot = zt_pop.reindex([original_model_col, split_factor_col], axis=1).groupby(
                original_model_col).sum().reset_index()

        # Derive adjustment for factor
        zt_pop_tot['adj_factor'] = 1/(zt_pop_tot[split_factor_col]/1)
        zt_pop_tot = zt_pop_tot.drop(split_factor_col, axis=1)
        zt_pop = zt_pop.merge(zt_pop_tot,
                              how='left',
                              on = original_model_col)
        zt_pop[split_factor_col] = zt_pop[split_factor_col] * zt_pop['adj_factor']
        zt_pop = zt_pop.drop('adj_factor', axis=1)

        # Round & drop 0 segments
        zt_emp[split_factor_col] = zt_emp[split_factor_col].round(3)
        zt_emp = zt_emp[zt_emp[split_factor_col]>0]

        # Correct factors back to 0
        zt_emp_tot = zt_emp.reindex([original_model_col,
                                     split_factor_col],
        axis=1).groupby(
                original_model_col).sum().reset_index()
        # Derive adjustment for factor
        zt_emp_tot['adj_factor'] = 1/(zt_emp_tot[split_factor_col]/1)
        zt_emp_tot = zt_emp_tot.drop(split_factor_col, axis=1)
        zt_emp = zt_emp.merge(zt_emp_tot,
                              how='left',
                              on = original_model_col)
        zt_emp[split_factor_col] = zt_emp[split_factor_col] * zt_emp['adj_factor']
        zt_emp = zt_emp.drop('adj_factor', axis=1)
        # zone translations defined

        # Rename destination zone names to join w/o duplication
        zt_pop = zt_pop.rename(
                columns={(other_model_name + '_zone_id'):'p_zone'})
        zt_emp = zt_emp.rename(
                columns={(other_model_name + '_zone_id'):'a_zone'})

        # Transform model demand to current model zones
        # Bring in population split for productions
        om_dist = om_dist.merge(zt_pop,
                                how='left',
                                on='p_zone')
        # Multiply out
        om_dist['dt'] = om_dist['dt'] * om_dist[('overlap_' + other_model_name + '_split_factor')]
        om_dist = om_dist.drop(
                ['p_zone',
                 'overlap_' + other_model_name + '_split_factor'], axis=1)
        om_dist = om_dist.rename(
                columns={(model_name.lower() + '_zone_id'):'p_zone'})
        # Group and sum
        group_cols = ['p_zone', 'a_zone']
        sum_cols = group_cols.copy()
        sum_cols.append('dt')

        om_dist = om_dist.reindex(
                sum_cols,axis=1).groupby(
                        group_cols).sum().reset_index()

        # Bring in employment split for attractions
        om_dist = om_dist.merge(zt_emp,
                                how='left',
                                on='a_zone')
        # Multiply out
        om_dist['dt'] = om_dist['dt'] * om_dist[('overlap_' + other_model_name + '_split_factor')]
        om_dist = om_dist.drop(
                ['a_zone',
                 'overlap_' + other_model_name + '_split_factor'], axis=1)
        om_dist = om_dist.rename(
                columns={(model_name.lower() + '_zone_id'):'a_zone'})
        # Group and sum
        om_dist = om_dist.reindex(
                sum_cols,axis=1).groupby(
                        group_cols).sum().reset_index()

        # Audit total
        if om_productions.round(0) == om_dist['dt'].sum().round(0):
            print('Balanced well')
        else:
            print('Balance off') # TODO: Bit more

        # Reduce to zonal factors to apply new productions to
        om_dist_p_totals = om_dist.reindex(['p_zone', 'dt'], axis=1).groupby('p_zone').sum().reset_index()
        om_dist_p_totals = om_dist_p_totals.rename(columns={'dt':'dt_total'})

        om_dist = om_dist.merge(om_dist_p_totals,
                                how='left',
                                on='p_zone')
        # Derive p/a share by p
        om_dist['p_a_share'] = om_dist['dt']/om_dist['dt_total']
        om_dist = om_dist.drop(['dt', 'dt_total'],axis=1)

        # Get target productions
        target_p = nup.filter_distribution_p(internal_24hr_productions,
                                             model_name.lower() + '_zone_id',
                                             calib_params,
                                             round_val=3)

        original_p = target_p['productions'].sum()
        target_p = target_p.rename(columns={target_model_col:'p_zone'})

        # Join on distribution
        target_p = target_p.merge(om_dist,
                                  how='left',
                                  on='p_zone')
        target_dist = target_p.copy()
        target_dist['dt'] = (target_dist['productions'] *
                   target_dist['p_a_share'])
        target_dist = target_dist.drop(['productions', 'p_a_share'], axis=1)

        # Correct for dropped trips as required
        correction_factor = 1/(target_dist['dt'].sum()/original_p)
        if correction_factor > 1.1 or correction_factor < 0.9:
            Warning('Massive correction required for balance, check inputs')
        target_dist['dt'] = target_dist['dt'] * correction_factor

        # Audit total
        if original_p.round(0) == target_dist['dt'].sum().round(0):
            print('Balanced well')
        else:
            print('Balance off') # TODO: Bit more

        # Reapply segments
        col_ph = ['p_zone', 'a_zone']
        for key, value in calib_params.items():
            col_ph.append(key)
            print('Reappending segment values for ' + key)
            target_dist[key] = value
        col_ph.append('dt')

        # Final order preserving reindex
        target_dist = target_dist.reindex(col_ph, axis=1)

        return(target_dist)


    def pass_to_intrazonal(internal_24hr_productions,
                           ia_name,
                           calib_params,
                           verbose=True):

        """
        Function to pass distributions straight to intrazonal as required.
        For active modes and PT until MSOA distribution in March-20
        """

        # Get unique internal zones in a smart way
        min_zone = min(internal_24hr_productions[ia_name])
        max_zone = max(internal_24hr_productions[ia_name])
        unq_internal_zones = [i for i in range(min_zone, max_zone+1)]

        target_p = nup.filter_distribution_p(internal_24hr_productions,
                                             ia_name,
                                             calib_params,
                                             round_val=3,
                                             verbose=verbose)[0]
        intra_dist = target_p
        intra_dist['p_zone'] = intra_dist[ia_name]
        intra_dist['a_zone'] = intra_dist[ia_name]
        intra_dist = intra_dist.drop(ia_name,
                                     axis=1)
        intra_dist = intra_dist.rename(columns={'productions': 'dt'})

        # Re-append values
        reindex_cols = ['p_zone', 'a_zone']
        for key, value in calib_params.items():
            intra_dist[key] = value
            reindex_cols.append(key)

        # Reindex cols to match standard
        reindex_cols.append('dt')
        intra_dist = intra_dist.reindex(
                reindex_cols, axis=1).reset_index(drop=True)

        # Push to wide format
        intra_dist = nup.df_to_np(intra_dist,
                                  v_heading='p_zone',
                                  h_heading='a_zone',
                                  values='dt',
                                  unq_internal_zones=unq_internal_zones,
                                  verbose=verbose)

        intra_dist = pd.DataFrame(intra_dist,
                                  index=unq_internal_zones,
                                  columns=unq_internal_zones)

        return intra_dist


    def run_separation(initial_betas, available_dists):

        """
        Function to separate intial betas based on the type of distribution required.

        null_dists = Distributions to be ignored (usually cjtw)
        intra_zonal_dists = Combos to be passed straight to intra zonal and exported
        zone_conversion_dists = Combos to be translated from another zoning system
        initial_betas = actual distribution betas

        """
        # Get len at the start
        beta_length_start = len(initial_betas.iloc[:, 0])

        null_dists = initial_betas[
            initial_betas['distribution'] == 'none'
        ].copy().reset_index(drop=True)

        cjtw_dists = initial_betas[
            initial_betas['distribution'] == 'cjtw'
        ].copy().reset_index(drop=True)

        intra_zonal_dists = initial_betas[
            initial_betas['distribution'] == 'intra'
        ].copy().reset_index(drop=True)

        translated_dists = initial_betas[
            initial_betas['distribution'].isin(available_dists)
        ].copy().reset_index(drop=True)

        initial_betas = initial_betas[
            initial_betas['distribution'] == 'synthetic'
        ].reset_index(drop=True)

        beta_length_end = sum([len(null_dists.iloc[:,0]),
                              len(cjtw_dists.iloc[:0]),
                              len(intra_zonal_dists.iloc[:,0]),
                              len(translated_dists.iloc[:,0]),
                              len(initial_betas.iloc[:,0])])
        if beta_length_start == beta_length_end:
            print('All betas accounted for')
        else:
            Warning('Some betas dropped')

        return(initial_betas,
               translated_dists,
               intra_zonal_dists,
               cjtw_dists,
               null_dists)


    def fetch_other_model_distribution(other_model_name,
                                       file_drive,
                                       available_dists,
                                       calib_params,
                                       distribution_segments,
                                       t_iteration=None,
                                       verbose=True):

        """
        Function to smartly path into another distribution, as specified in
        a set of target betas.
        Returns a distribution in long format (internal 24hr PA)
        """
        # TODO: Add ability to take dist from fusion output!

        l_available_dists = [x.lower() for x in available_dists]

        t_import_source = other_model_name
        t_import_path = os.path.join(file_drive,
                                     'NorMITs Synthesiser',
                                     available_dists[l_available_dists.index(t_import_source)])

        # Find largest iter
        if t_iteration is None:
            t_import_dir = os.listdir(t_import_path)
            t_iter_list = []
            for item in t_import_dir:
                if len(item) < 7 and item.startswith('iter'):
                    if item == 'iterx':
                        break
                    else:
                        t_iter_list.append(item)
                     # Remove iterx
            t_iteration = max(t_iter_list)
        else:
            nup.print_w_toggle('Checking for ' + str(t_iteration), verbose=verbose)

        t_import_path = os.path.join(t_import_path,
                                     t_iteration,
                                     'Distribution Outputs',
                                     '24hr PA Distributions')

        # Try and get a matching distribution from the folder
        # TODO: Update to lookup by segment, or at least try
        t_import_dir = os.listdir(t_import_path)

        cal_mode = ('mode_' + str(calib_params['m']))
        cal_purpose = ('purpose_' + str(calib_params['p']))
        t_import_dist = [x for x in t_import_dir if cal_mode in x]
        t_import_dist = [x for x in t_import_dist if cal_purpose in x]

        # If more than 1, try segments!
        # Drop mode and purpose from segments
        other_segments = distribution_segments.copy()
        other_segments.remove('m')
        other_segments.remove('p')

        if len(t_import_dist) > 1:
            t_import_dist_seg = t_import_dist
            # Check if we can make a match to segmented data
            for segment in other_segments:
                cal_seg = (segment + '_' + str(calib_params[segment]))
                t_import_dist_seg = [x for x in t_import_dist_seg if cal_seg in x]
            # If yes use that going forward
            if len(t_import_dist_seg) == 1:
                t_import_dist = t_import_dist_seg
                # TODO: Smarter append to ignored segment list
                # ignored_segs = None
        else:
            nup.print_w_toggle('Ignored segs on reimport ', other_segments, verbose=verbose)
            # ignored_segs = other_segments.copy()

        dist_ph = []
        for item in t_import_dist:
            temp_dist = pd.read_csv(os.path.join(t_import_path, item))
            temp_dist = nup.optimise_data_types(temp_dist, verbose=verbose)
            dist_ph.append(temp_dist)
            del(temp_dist)

        t_dist = pd.concat(dist_ph, sort=True)

        # Used to return ignored segs but I don't want it to anymore
        return t_import_dist, t_dist


    def _synth_dist_worker(
            self,
            dist_index,
            synthetic_dists,
            segmentation,
            distribution_segments,
            dist_function,
            tlb_area,
            trip_origin,
            target_trip_lengths,
            ia_name,
            productions,
            attractions,
            cost_type,
            furness_loops,
            fitting_loops,
            i_paths,
            o_paths,
            iz_cost_infill,
            verbose=True):

        """
        Distributes the synthetic distributions, producing 24hr PA Matrices

        Encapsulates the inner workings of the synthetic distribution loop from
        run_distribution_model(). Useful for use in multi-threaded environments
        """
        # get all segments and define parameters for distribution
        calib_params = {}
        for ds in distribution_segments:
            calib_params.update({ds: target_trip_lengths[ds][dist_index]})

        # Get initial alpha & beta from distribution parameters
        distribution_params = self.get_distribution_parameters(
            synthetic_dists, calib_params)

        # Print a message telling the user which matrix is being worked on
        dist_name = nup.generate_distribution_name(calib_params)
        print("INFO: Creating the synthetic distribution for %s..." % dist_name)

        # Unpack params into dict
        init_param_a = distribution_params['init_param_a']
        init_param_b = distribution_params['init_param_b']

        del distribution_params

        # Work out trip length band path from given params
        # Only want standard segments at this point
        tlb_folder = os.path.join(i_paths['imports'],
                                  'trip_length_bands',
                                  tlb_area,
                                  'standard_segments')

        # Get trip length bands
        tlb = nup.get_trip_length_bands(tlb_folder,
                                        calib_params,
                                        segmentation,
                                        trip_origin=trip_origin,
                                        replace_nan=True,
                                        verbose=verbose)

        calib_params.update({'tlb': tlb})

        # Loop over the distributions until beta gives:
        # 1. a decent average trip length by band
        # 2. decent proportions within the segments

        # TODO: Filter should be done outside of function and passed as np vector, still inside atm
        #  - this applies to intra & cjtw too
        hb_distribution = gm.run_gravity_model(
                ia_name,
                calib_params,
                init_param_a,
                init_param_b,
                productions,
                attractions,
                model_lookup_path=i_paths['lookups'],
                dist_log_path=o_paths['reports'],
                dist_log_fname=trip_origin + '_internal_distribution',
                dist_function = dist_function,
                cost_type=cost_type,
                apply_k_factoring = True,
                furness_loops=furness_loops,
                fitting_loops=fitting_loops,
                bs_con_target = .95,
                target_r_gap = 1,
                rounding_factor=3,
                iz_cost_infill=iz_cost_infill,
                verbose=verbose
        )


        # Print calib outputs - append calib outputs


        # Export 24hr distribution
        # Generate distribution name
        dist_path = os.path.join(o_paths['summaries'],
                                 trip_origin + '_synthetic')

        dist_path = nup.build_path(dist_path, calib_params)

        # Generate trip length bin name
        bin_path = os.path.join(o_paths['tld'],
                                trip_origin + '_synthetic_bin')

        bin_path = nup.build_path(bin_path, calib_params)

        # Generate trip length band name
        tlb_path = os.path.join(o_paths['reports'],
                                trip_origin + '_trip_length_bands')

        tlb_path = nup.build_path(tlb_path, calib_params)

        # Generate band distribution name
        tbd_path = os.path.join(o_paths['reports'],
                                trip_origin + '_band_distribution')

        tbd_path = nup.build_path(tbd_path, calib_params)

        # Export distribution
        hb_distribution[0].to_csv(dist_path, index=False)
        # Export trip length bins
        hb_distribution[1].to_csv(bin_path, index=False)

        del hb_distribution

    def _trans_dist_worker(
            self,
            dist_index,
            translated_dists,
            available_dists,
            model_name,
            distribution_segments,
            trip_origin,
            internal_24hr_productions,
            i_paths,
            o_paths,
            verbose=True):
        """
        Distributes the translated distributions, producing 24hr PA Matrices

        Encapsulates the inner workings of the translated distribution loop from
        run_distribution_model(). Useful for use in multi-threaded environments
        """
        # TODO: make sure verbose=False it actually suppresses all prints
        calib_params = {}
        for ds in distribution_segments:
            calib_params.update({ds: translated_dists[ds][dist_index]})

        # Print a message telling the user which matrix is being worked on
        dist_name = nup.generate_distribution_name(calib_params)
        print("INFO: Creating the synthetic distribution for %s..." % dist_name)

        # Get distribution from import source
        # TODO: tighten up calls to other dists
        other_model_name = translated_dists['distribution'][dist_index]
        omd = self.fetch_other_model_distribution(
            other_model_name,
            available_dists,
            calib_params,
            distribution_segments,
            t_iteration='iter4',
            verbose=verbose)

        om_name = omd[0]
        nup.print_w_toggle('Using ' + str(om_name), verbose=verbose)
        om_dist = omd[1]
        # om_ignored_segs = omd[2]
        # print('This distribution ignoring ' + str(om_ignored_segs) +
        #       ' this may mean distribution ignores segment costs')

        del omd
        source_trips = om_dist['dt'].sum()
        nup.print_w_toggle(
            'Source trips: ' + str(source_trips), verbose=verbose)

        t_distribution = self.translate_distribution(
            om_dist,
            model_name,
            other_model_name,
            internal_24hr_productions,
            calib_params,
            target_path=i_paths['lookups'])

        target_trips = t_distribution['dt'].sum()
        nup.print_w_toggle('Target trips: ' + str(target_trips), verbose=verbose)

        # Export translated distribution
        t_dist_path = os.path.join(o_paths['summaries'],
                                   trip_origin + '_synthetic')
        t_dist_path = nup.build_path(t_dist_path, calib_params)

        t_distribution.to_csv(t_dist_path, index=False)

    def _iz_dist_worker(
            self,
            dist_index,
            intra_zonal_dists,
            distribution_segments,
            trip_origin,
            internal_24hr_productions,
            ia_name,
            o_paths,
            verbose=True):
        """
        Distributes the intra_zonal distributions, producing 24hr PA Matrices

        Encapsulates the inner workings of the intra_zonal distribution loop from
        run_distribution_model(). Useful for use in multi-threaded environments
        """
        calib_params = {}
        for ds in distribution_segments:
            calib_params.update({ds: intra_zonal_dists[ds][dist_index]})

        # Print a message telling the user which matrix is being worked on
        dist_name = nup.generate_distribution_name(calib_params)
        print("INFO: Creating the synthetic distribution for %s..." % dist_name)

        iz_distribution = self.pass_to_intrazonal(
            internal_24hr_productions,
            ia_name,
            calib_params,
            verbose=verbose)

        # Export translated distribution
        iz_dist_path = os.path.join(o_paths['summaries'],
                                    trip_origin + '_synthetic')
        iz_dist_path = nup.build_path(iz_dist_path, calib_params)

        iz_distribution.to_csv(iz_dist_path)

    def _cjtw_dist_worker(
            self,
            dist_index,
            cjtw_dists,
            model_name,
            distribution_segments,
            trip_origin,
            internal_24hr_productions,
            cost_type,
            ia_name,
            i_paths,
            o_paths,
            verbose=True):
        """
        Distributes the cjtw distributions, producing 24hr PA Matrices

        Encapsulates the inner workings of the cjtw distribution loop from
        run_distribution_model(). Useful for use in multi-threaded environments
        """

        # TODO: Get to work in same way as others
        calib_params = {}
        for ds in distribution_segments:
            calib_params.update({ds: cjtw_dists[ds][dist_index]})

        # Print a message telling the user which matrix is being worked on
        dist_name = nup.generate_distribution_name(calib_params)
        print("INFO: Creating the synthetic distribution for %s..." % dist_name)

        # Doesn't look like this is used?
        # tlb_folder = os.path.join(i_paths['imports'],
        #                           'trip_length_bands',
        #                           tlb_area,
        #                           'standard_segments')
        # Get trip length bands
        # tlb = nup.get_trip_length_bands(tlb_folder,
        #                                 calib_params,
        #                                 segmentation,
        #                                 trip_origin = trip_origin)

        if cost_type != '24hr':
            print('WARNING: Costs are not 24hr, trip length band audit may not work.')

        cjtw_distribution = self.distribute_cjtw(
            internal_24hr_productions,
            i_paths['lookups'],
            ia_name,
            model_name,
            calib_params,
            cost_type=cost_type,
            subset=None,
            verbose=verbose)

        # Export 24hr distribution
        # Generate distribution name
        c_dist_path = os.path.join(o_paths['summaries'],
                                   trip_origin + '_synthetic')
        c_dist_path = nup.build_path(c_dist_path, calib_params)

        # Generate trip length bin name
        c_bin_path = os.path.join(o_paths['tld'],
                                  trip_origin + '_synthetic_bin')
        c_bin_path = nup.build_path(c_bin_path, calib_params)

        # Export distribution
        cjtw_distribution[0].to_csv(c_dist_path, index=True)
        # Export trip length bins
        cjtw_distribution[1].to_csv(c_bin_path, index=False)


    def run_distribution_model(
            self,
            trip_origin='hb',
            cost_type='24hr',
            furness_loops=1999,
            fitting_loops=100,
            iz_cost_infill=.5,
            export_modes=[3],
            verbose=True,
            mp_threads=-1):

        """
        This function runs the distribution of productions and attractions to a
        24hr PA matrix.

        Parameters
        ----------
        production_import_path=_default_p_import_path:
            Import path for productions.

        attraction_import_path=_default_a_import_path:
            Import path for attractions.

        model_folder:
            Folder containing the relevant meta data for the desired zoning
            system.

        iteration:
            Iteration of current run. This will be in output names and also the
            target folder.

        segmentation = 'tfn':
            target segmentation, takes 'ntem' or 'tfn'

        distribution_segments = ['purpose', 'mode', 'car_availability']:
            Segments to distribute to at 24hr PA. D

        distribution_method = 'tanner':
            distribution method for gravity model - takes 'tanner' which works and
            soon to take 'ln' as log normal.

        verbose:
            Indicates whether to print a log of the process to the terminal.
            Useful to set verbose=False when using multi-threaded loops.
            Defaults to True.

        mp_threads:
            The number of threads to use when multiprocessing.
            Set mp_threads=1 to not use multiple threads.
            Set mp_threads=0 to not use multi-threaded code at all.
            Defaults to 0.

        Returns
        ----------
        pa_24hr:
            24hr PA matrix by zone by mode and purpose.
        """
        # Argument validation
        assert (mp_threads >= -1), ("In distribution_model.run_distribution_model(), " +
                                    "mp_threads must be -1 or greater.\n" +
                                    "Got %s." % str(mp_threads))
        if mp_threads == -1:
            mp_threads = os.cpu_count()-1

        # Path to root, stop any folder overwrites
        # Define list of available distributions (for translation)
        # TODO: filter out fluff, lower case
        # TODO: Push to run script
        available_dists = os.listdir(os.path.join(file_drive, 'NorMITs Synthesiser/'))

        # Paths returns:
        # (import_folder_path, model_lookup_path,
        # p_import_path, a_import_path, export_list)

        print('working folder:' + os.getcwd())
        print('importing global params from:' + i_paths['imports'])
        print('lookups from:' + i_paths['lookups'])
        print('productions from:' + i_paths['production_import'])
        print('attractions from:' + i_paths['attraction_import'])

        # Import productions and attractions
        pa = nup.import_pa(i_paths['production_import'], # p import path
                           i_paths['attraction_import']) # a import path
        productions = pa[0]
        attractions = pa[1]
        del(pa)

        ia_areas = nup.define_internal_external_areas(i_paths['lookups'])
        internal_area = ia_areas[0]
        ia_name = list(internal_area)[0]
        external_area = ia_areas[1]

        ### TODO: Distribution model is an object and this is the end of the _init_
        ### End of production core, ready for all homebased runs.

        # TODO: Build the init_params, if there aren't any

        # Get initial betas
        init_params = nup.get_init_params(i_paths['lookups'],
                                          distribution_type=trip_origin,
                                          model_name=model_name,
                                          mode_subset=None,
                                          purpose_subset=None)

        # null_dists = Distributions to be ignored (usually cjtw)
        # intra_zonal_dists = Combos to be passed straight to intra zonal and exported
        # zone_conversion_dists = Combos to be translated from another zoning system
        # init_params = actual distribution alphas & betas

        segmented_params = run_separation(init_params,
                                          # lower dist list
                                          [x.lower() for x in available_dists])

        synthetic_dists = segmented_params[0]
        translated_dists = segmented_params[1] # TODO: Specify iteration
        intra_zonal_dists = segmented_params[2]
        cjtw_dists = segmented_params[3]
        null_dists = segmented_params[4]

        # Get target trip length
        ttl_cols = distribution_segments.copy()
        ttl_cols.append('average_trip_length')

        # Target trip lengths here, derived straight from beta file
        target_trip_lengths = synthetic_dists.reindex(
                ttl_cols,
                axis=1).reset_index(drop=True)

        # Let the user know if threads are being used or not
        if mp_threads == 0:
            print("INFO: mp_threads set to 0, running as basic loops.")
        elif mp_threads == 1:
            print("INFO: mp_threads set to 1, using a single thread.")
        else:
            print("INFO: mp_threads set to 2 or greater, using multiple threads.")

        # Iterate over distributions
        print("-" * 20, " Start Synthetic Distributions ", "-" * 20)
        # Build a dictionary of unchanging keyword arguments for each function call
        unchanging_kwargs = {'synthetic_dists': synthetic_dists,
                             'segmentation': segmentation,
                             'distribution_segments': distribution_segments,
                             'dist_function': dist_function,
                             'tlb_area': tlb_area,
                             'trip_origin': trip_origin,
                             'target_trip_lengths': target_trip_lengths,
                             'ia_name': ia_name,
                             'productions': productions,
                             'attractions': attractions,
                             'cost_type': cost_type,
                             'furness_loops':furness_loops,
                             'fitting_loops':fitting_loops,
                             'i_paths': i_paths,
                             'o_paths': o_paths,
                             'iz_cost_infill': iz_cost_infill,
                             'verbose': verbose}

        if mp_threads == 0:
            for i in target_trip_lengths.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                print('Running ' + str(i))
                self._synth_dist_worker(**kwargs)
        else:
            # Build a list of kwargs for each function call
            kwargs_list = list()
            for i in target_trip_lengths.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                kwargs_list.append(kwargs)

            # Call using multiple threads
            mp.process_pool_wrapper(self._synth_dist_worker,
                                    kwargs=kwargs_list,
                                    process_count=mp_threads)
        del unchanging_kwargs
        print("-" * 20, " End Synthetic Distributions ", "-" * 20)

        print("-" * 20, " Start Translated Distributions ", "-" * 20)

        # Build a dictionary of unchanging keyword arguments for each function call
        unchanging_kwargs = {'translated_dists': translated_dists,
                             'available_dists': available_dists,
                             'model_name': model_name,
                             'distribution_segments': distribution_segments,
                             'trip_origin': trip_origin,
                             'internal_24hr_productions': productions,
                             'i_paths': i_paths,
                             'o_paths': o_paths,
                             'file_drive': file_drive,
                             'verbose': verbose}

        if mp_threads == 0:
            for i in translated_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                self._trans_dist_worker(**kwargs)
        else:
            # Build a list of kwargs for each function call
            kwargs_list = list()
            for i in translated_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                kwargs_list.append(kwargs)

            # Call using multiple threads
            mp.process_pool_wrapper(self._trans_dist_worker,
                                    kwargs=kwargs_list,
                                    process_count=mp_threads)
        del unchanging_kwargs
        print("-" * 20, " End Translated Distributions ", "-" * 20)

        print("-" * 20, " Start Intra-Zonal Distributions ", "-" * 20)
        # Build a dictionary of unchanging keyword arguments for each function call
        unchanging_kwargs = {'intra_zonal_dists': intra_zonal_dists,
                             'distribution_segments': distribution_segments,
                             'trip_origin': trip_origin,
                             'internal_24hr_productions': productions,
                             'ia_name': ia_name,
                             'o_paths': o_paths,
                             'verbose': verbose}

        if mp_threads == 0:
            for i in intra_zonal_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                self._iz_dist_worker(**kwargs)
        else:
            # Build a list of kwargs for each function call
            kwargs_list = list()
            for i in intra_zonal_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                kwargs_list.append(kwargs)

            # Call using multiple threads
            mp.process_pool_wrapper(self._iz_dist_worker,
                                    kwargs=kwargs_list,
                                    process_count=mp_threads)
        del unchanging_kwargs
        print("-" * 20, " End Intra-Zonal Distributions ", "-" * 20)

        print("-" * 20, " Start CJTW Distributions ", "-" * 20)
        # Build a dictionary of unchanging keyword arguments for each function call
        unchanging_kwargs = {'cjtw_dists': cjtw_dists,
                             'model_name': model_name,
                             'distribution_segments': distribution_segments,
                             'trip_origin': trip_origin,
                             'internal_24hr_productions': productions,
                             'cost_type': cost_type,
                             'ia_name': ia_name,
                             'i_paths': i_paths,
                             'o_paths': o_paths,
                             'verbose': verbose}

        if mp_threads == 0:
            for i in cjtw_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                self._cjtw_dist_worker(**kwargs)
        else:
            # Build a list of kwargs for each function call
            kwargs_list = list()
            for i in cjtw_dists.index:
                kwargs = unchanging_kwargs.copy()
                kwargs['dist_index'] = i
                kwargs_list.append(kwargs)

            # Call using multiple threads
            mp.process_pool_wrapper(self._cjtw_dist_worker,
                                    kwargs=kwargs_list,
                                    process_count=mp_threads)
        del unchanging_kwargs
        print("-" * 20, " End CJTW Distributions ", "-" * 20)
        print('Done!')
